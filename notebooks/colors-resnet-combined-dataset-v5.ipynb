{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow Version: 5.4.1\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "print('Pillow Version:', PIL.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.preprocessing.image as preprocess\n",
    "from DataGenerator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two blocks contain parameters that you can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing images\n",
    "#path = \"/home/hippoc/schen/imgs_colors/\"\n",
    "path = \"/home/hippoc/bin/beauvoir/data/thin_distract_plus/dataset/\" # path where the training/validations sets are stored\n",
    "test_path = \"/home/hippoc/bin/beauvoir/data/crop_data/300x200/test/combined/\" # path where the test set is stored\n",
    "\n",
    "#image_num = 8000\n",
    "#images = np.zeros((image_num, 200, 300, 3))\n",
    "#for i in range(image_num):\n",
    "#    img = preprocess.load_img(path + str(i)+'.png')\n",
    "#    img_array = preprocess.img_to_array(img)\n",
    "#    images[i] = img_array\n",
    "    \n",
    "# preprocessing images (range from 0 to 1)\n",
    "#images /= 255.0\n",
    "\n",
    "#datagen = ImageDataGenerator(\n",
    "#        featurewise_center=True,\n",
    "#        samplewise_center=False,\n",
    "#        featurewise_std_normalization=True,\n",
    "#        samplewise_std_normalization=False,\n",
    "#        zca_whitening=True,\n",
    "#        rotation_range=0,\n",
    "#        width_shift_range=0,\n",
    "#        height_shift_range=0,\n",
    "#        horizontal_flip=False,\n",
    "#        vertical_flip=False\n",
    "#        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (200, 300, 3) # (height, width, channels) of images\n",
    "\n",
    "image_num = 140800\n",
    "training_labels = (0, 130800) # the image range for training\n",
    "validation_labels = (130800, 140800) # the image range for validation\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': input_shape[0:2],\n",
    "          'batch_size': 32,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next blocks are for setting up the model and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {}\n",
    "partition['train'] = list(range(training_labels[0], training_labels[1]))\n",
    "partition['validation'] = list(range(validation_labels[0], validation_labels[1]))\n",
    "\n",
    "# importing labels\n",
    "f = open(path + 'labels.txt', 'r')\n",
    "labels = f.readlines()\n",
    "labels = [eval(x.strip()) for x in labels]\n",
    "\n",
    "# change labels to range from 0-num of pixels\n",
    "labels = [(x[0] * 300, x[1] * 200) for x in labels]\n",
    "\n",
    "# preprocessing labels (range from -1 to 1)\n",
    "labels_orig = np.round(np.array([[tup[0], tup[1]] for tup in labels]))\n",
    "labels = [[(tup[0]-(input_shape[1]/2))/(input_shape[1]/2), (tup[1]-(input_shape[0]/2))/-(input_shape[0]/2)] for tup in labels]\n",
    "labels = np.array(labels)\n",
    "labels_dict = {}\n",
    "for i in range(image_num):\n",
    "    labels_dict[i] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03295458 -0.        ]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# testing that labels properly imported\n",
    "print(labels_dict[152])\n",
    "print(labels_dict[152].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels_dict, path=path, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels_dict, path=path, **params)\n",
    "\n",
    "# testing that images loaded correctly\n",
    "#plt.imshow(images[0])\n",
    "\n",
    "#print(labels)\n",
    "#X, y, indexes = training_generator.__getitem__(0)\n",
    "#print(y[0])\n",
    "#print(labels_dict[indexes[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, Input, BatchNormalization, Activation\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import History, ModelCheckpoint\n",
    "\n",
    "def ResLayer(x, filters):\n",
    "    # identity\n",
    "    res = x\n",
    "    res = BatchNormalization()(res)\n",
    "    res = Conv2D(filters=filters, kernel_size=[1,1], strides=2, padding='same', use_bias=False)(res)\n",
    "    \n",
    "    # conv layers\n",
    "    out = BatchNormalization()(x)\n",
    "    out = Activation('relu')(out)\n",
    "#    out = Activation('relu')(x)\n",
    "    out = Conv2D(filters=filters, kernel_size=[3,3], strides=2, padding='same', use_bias=False)(out)\n",
    "    \n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Conv2D(filters=filters, kernel_size=[3,3], strides=1, padding='same', use_bias=False)(out)\n",
    "    \n",
    "    # adding the identity\n",
    "    out = keras.layers.add([res,out])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 100, 150, 32) 2432        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 49, 74, 32)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 49, 74, 32)   128         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 49, 74, 32)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 25, 37, 32)   9216        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 25, 37, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 49, 74, 32)   128         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 25, 37, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 25, 37, 32)   1024        batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 25, 37, 32)   9216        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 25, 37, 32)   0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 25, 37, 32)   128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 25, 37, 32)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 13, 19, 64)   18432       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 13, 19, 64)   256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 25, 37, 32)   128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 13, 19, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 13, 19, 64)   2048        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 13, 19, 64)   36864       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 13, 19, 64)   0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 13, 19, 64)   256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 13, 19, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 10, 128)   73728       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 7, 10, 128)   512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 13, 19, 64)   256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 7, 10, 128)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 7, 10, 128)   8192        batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 10, 128)   147456      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 7, 10, 128)   0           conv2d_28[0][0]                  \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 7, 10, 128)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 7, 10, 128)   0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8960)         0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            17922       flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 328,450\n",
      "Trainable params: 327,490\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(filters=32, kernel_size=[5,5], strides=2, padding='same')(inputs)\n",
    "x = MaxPooling2D(pool_size=(3,3), strides=2)(x)\n",
    "\n",
    "x = ResLayer(x, 32)\n",
    "x = ResLayer(x, 64)\n",
    "x = ResLayer(x, 128)\n",
    "\n",
    "x = Dropout(0.5)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(2)(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def mean_mse(y_true, y_pred):\n",
    "    return K.mean(K.sum(K.square(y_true-y_pred), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss history\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "history = LossHistory()\n",
    "\n",
    "mc = ModelCheckpoint('/home/hippoc/schen/models/v6/weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4087/4087 [==============================] - 3001s 734ms/step - loss: 2049.1761 - mean_mse: 0.0637 - val_loss: 108.4954 - val_mean_mse: 0.0436\n",
      "\n",
      "Epoch 00001: saving model to /home/hippoc/schen/models/v6/weights.01-108.50.hdf5\n",
      "Epoch 2/10\n",
      "4087/4087 [==============================] - 2968s 726ms/step - loss: 1409.3765 - mean_mse: 0.0433 - val_loss: 108.3740 - val_mean_mse: 0.0435\n",
      "\n",
      "Epoch 00002: saving model to /home/hippoc/schen/models/v6/weights.02-108.37.hdf5\n",
      "Epoch 3/10\n",
      "3017/4087 [=====================>........] - ETA: 17:50 - loss: 1409.1527 - mean_mse: 0.0433"
     ]
    }
   ],
   "source": [
    "# actual model fitting using tuned hyperparameters\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.0004508087977693204\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=Adam(lr=lr),\n",
    "              metrics=[mean_mse]\n",
    "              )\n",
    "\n",
    "hist = model.fit_generator(generator=training_generator,\n",
    "                           validation_data=validation_generator,\n",
    "                           use_multiprocessing=False,\n",
    "                           epochs=epochs,\n",
    "                           callbacks=[history, mc],\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot((np.array(range(len(history.losses))))/epochs, history.losses)\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next blocks are for testing the model. They make predictions on images and then label images with a cross so that you can see whether or not the model is making good predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# making annotations on validation data\n",
    "\n",
    "#image_num = validation_labels[1] - validation_labels[0]\n",
    "image_num = 100\n",
    "images = np.zeros((image_num, input_shape[0], input_shape[1], input_shape[2]))\n",
    "for i in range(image_num):\n",
    "    img = preprocess.load_img(path + str(i+validation_labels[0])+'.png')\n",
    "    img_array = preprocess.img_to_array(img)\n",
    "    images[i] = img_array\n",
    "    \n",
    "# preprocessing images (range from 0 to 1)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "x_pred = images\n",
    "y_pred = labels[validation_labels[0]:validation_labels[0]+image_num]\n",
    "preds = model.predict(x_pred, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,0] = preds[:,0]*150 + 150\n",
    "preds[:,1] = preds[:,1]*-100 + 100\n",
    "preds = np.round(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[160. 115.] [157. 111.] [3. 4.]\n",
      "[209.  48.] [150. 101.] [ 59. -53.]\n",
      "[96. 81.] [106.  78.] [-10.   3.]\n",
      "[161.  97.] [148. 100.] [13. -3.]\n",
      "[ 79. 141.] [ 79. 128.] [ 0. 13.]\n",
      "[184.  70.] [179.  68.] [5. 2.]\n",
      "[152.  49.] [153.  56.] [-1. -7.]\n",
      "[145.  67.] [145.  75.] [ 0. -8.]\n",
      "[223.  70.] [189.  80.] [ 34. -10.]\n",
      "[148. 116.] [149. 117.] [-1. -1.]\n",
      "[ 80. 153.] [ 95. 151.] [-15.   2.]\n",
      "[167.  74.] [161.  82.] [ 6. -8.]\n",
      "[166. 121.] [168. 115.] [-2.  6.]\n",
      "[119. 138.] [117. 130.] [2. 8.]\n",
      "[147. 102.] [151. 104.] [-4. -2.]\n",
      "[216.  76.] [204.  83.] [12. -7.]\n",
      "[69. 92.] [80. 94.] [-11.  -2.]\n",
      "[184.  85.] [179.  89.] [ 5. -4.]\n",
      "[135.  98.] [138.  99.] [-3. -1.]\n",
      "[193. 118.] [195. 113.] [-2.  5.]\n",
      "[151. 110.] [146. 107.] [5. 3.]\n",
      "[165. 108.] [162. 104.] [3. 4.]\n",
      "[146.  82.] [146.  86.] [ 0. -4.]\n",
      "[221.  64.] [216.  76.] [  5. -12.]\n",
      "[150. 127.] [152. 123.] [-2.  4.]\n",
      "[140.  86.] [142.  90.] [-2. -4.]\n",
      "[121.  49.] [126.  53.] [-5. -4.]\n",
      "[198.  97.] [193.  98.] [ 5. -1.]\n",
      "[87. 88.] [104.  91.] [-17.  -3.]\n",
      "[115.  90.] [123.  93.] [-8. -3.]\n",
      "[163. 101.] [164. 100.] [-1.  1.]\n",
      "[258. 123.] [236. 117.] [22.  6.]\n",
      "[162.  86.] [160.  91.] [ 2. -5.]\n",
      "[142. 150.] [152. 136.] [-10.  14.]\n",
      "[148. 112.] [152. 108.] [-4.  4.]\n",
      "[242. 151.] [235. 138.] [ 7. 13.]\n",
      "[108.  74.] [114.  81.] [-6. -7.]\n",
      "[114.  87.] [126.  88.] [-12.  -1.]\n",
      "[104. 118.] [106. 112.] [-2.  6.]\n",
      "[159. 111.] [162. 110.] [-3.  1.]\n",
      "[129. 110.] [141. 107.] [-12.   3.]\n",
      "[162. 101.] [166. 103.] [-4. -2.]\n",
      "[201.  89.] [195.  85.] [6. 4.]\n",
      "[183. 113.] [176. 108.] [7. 5.]\n",
      "[191. 108.] [181. 110.] [10. -2.]\n",
      "[136.  82.] [136.  85.] [ 0. -3.]\n",
      "[170.  64.] [164.  75.] [  6. -11.]\n",
      "[147.  79.] [148.  81.] [-1. -2.]\n",
      "[167.  44.] [160.  61.] [  7. -17.]\n",
      "[176. 100.] [168. 104.] [ 8. -4.]\n",
      "[161. 110.] [164. 109.] [-3.  1.]\n",
      "[44. 67.] [66. 77.] [-22. -10.]\n",
      "[101. 131.] [110. 114.] [-9. 17.]\n",
      "[132.  63.] [135.  74.] [ -3. -11.]\n",
      "[164.  99.] [159.  97.] [5. 2.]\n",
      "[122.  71.] [123.  76.] [-1. -5.]\n",
      "[168. 100.] [166. 102.] [ 2. -2.]\n",
      "[156.  63.] [156.  70.] [ 0. -7.]\n",
      "[131.  51.] [142.  65.] [-11. -14.]\n",
      "[216.  79.] [212.  84.] [ 4. -5.]\n",
      "[170. 102.] [162. 100.] [8. 2.]\n",
      "[211. 170.] [190. 145.] [21. 25.]\n",
      "[119. 100.] [128.  98.] [-9.  2.]\n",
      "[162. 102.] [158. 103.] [ 4. -1.]\n",
      "[78. 99.] [100. 106.] [-22.  -7.]\n",
      "[163.  99.] [163. 103.] [ 0. -4.]\n",
      "[143. 113.] [143. 109.] [0. 4.]\n",
      "[146. 126.] [147. 118.] [-1.  8.]\n",
      "[139. 147.] [136. 141.] [3. 6.]\n",
      "[139.  85.] [148. 110.] [ -9. -25.]\n",
      "[115.  91.] [120.  95.] [-5. -4.]\n",
      "[137.  59.] [145.  67.] [-8. -8.]\n",
      "[147. 101.] [125.  90.] [22. 11.]\n",
      "[153.  90.] [149.  95.] [ 4. -5.]\n",
      "[167. 101.] [161. 100.] [6. 1.]\n",
      "[179. 131.] [175. 136.] [ 4. -5.]\n",
      "[158. 104.] [158. 111.] [ 0. -7.]\n",
      "[136. 132.] [141. 130.] [-5.  2.]\n",
      "[153.  98.] [145.  99.] [ 8. -1.]\n",
      "[148. 125.] [152. 119.] [-4.  6.]\n",
      "[153. 117.] [155. 111.] [-2.  6.]\n",
      "[135.  67.] [137.  76.] [-2. -9.]\n",
      "[157. 127.] [157. 119.] [0. 8.]\n",
      "[145.  57.] [144.  58.] [ 1. -1.]\n",
      "[168.  90.] [163.  93.] [ 5. -3.]\n",
      "[128.  83.] [132.  85.] [-4. -2.]\n",
      "[152. 167.] [144. 157.] [ 8. 10.]\n",
      "[178. 100.] [169. 102.] [ 9. -2.]\n",
      "[146. 103.] [140. 105.] [ 6. -2.]\n",
      "[122. 104.] [128. 104.] [-6.  0.]\n",
      "[155. 113.] [154. 108.] [1. 5.]\n",
      "[197.  69.] [174.  78.] [23. -9.]\n",
      "[154.  89.] [152.  93.] [ 2. -4.]\n",
      "[150. 106.] [148. 100.] [2. 6.]\n",
      "[164. 110.] [161. 104.] [3. 6.]\n",
      "[164.  79.] [166.  81.] [-2. -2.]\n",
      "[152.  63.] [152.  74.] [  0. -11.]\n",
      "[156.  97.] [154.  99.] [ 2. -2.]\n",
      "[129. 125.] [137. 120.] [-8.  5.]\n",
      "[123.  94.] [132.  95.] [-9. -1.]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "for i in range(validation_labels[0],validation_labels[0]+image_num):\n",
    "    img = Image.open(path + str(i)+'.png')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    pred = preds[i-validation_labels[0]]\n",
    "    \n",
    "    draw.line([tuple(pred - [10, 0]), tuple(pred + [10, 0])], fill=\"blue\", width=3)\n",
    "    draw.line([tuple(pred - [0, 10]), tuple(pred + [0, 10])], fill=\"red\", width=3)\n",
    "\n",
    "#    draw.point(labels_orig[i], fill=\"blue\")\n",
    "#    draw.point(preds[i-15840], fill=\"black\")\n",
    "    print(labels_orig[i], pred, labels_orig[i]- pred)\n",
    "    img.save(path + \"preds_cpu/\" + str(i) + \"_pred.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0730 15:42:58.861174 139782654879488 alias.py:221] Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "E0730 15:42:59.076416 139782654879488 alias.py:221] Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "E0730 15:42:59.087614 139782654879488 alias.py:221] Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "E0730 15:42:59.090473 139782654879488 alias.py:221] Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x7f218cf4f8c8> (for post_execute):"
     ]
    }
   ],
   "source": [
    "# making predictions on test data\n",
    "\n",
    "test_image_num = 5000\n",
    "\n",
    "test_images = np.zeros((test_image_num, input_shape[0], input_shape[1], input_shape[2]))\n",
    "for i in range(test_image_num):\n",
    "    img = preprocess.load_img(test_path + str(i)+'.png')\n",
    "    img_array = preprocess.img_to_array(img)\n",
    "    test_images[i] = img_array\n",
    "    \n",
    "# preprocessing images (range from 0 to 1)\n",
    "test_images /= 255.0\n",
    "\n",
    "\n",
    "test_x_pred = test_images\n",
    "\n",
    "\n",
    "# importing test labels\n",
    "f = open(test_path + 'labels.txt', 'r')\n",
    "test_labels = f.readlines()\n",
    "test_labels = [eval(x.strip()) for x in test_labels]\n",
    "\n",
    "# change labels to range from 0-num of pixels\n",
    "test_labels_pics = [(x[0] * 300, x[1] * 200) for x in test_labels]\n",
    "\n",
    "test_y_pred = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions on test data\n",
    "test_preds = model.predict(test_x_pred, verbose=1)\n",
    "\n",
    "# converting to pixel locations\n",
    "test_preds[:,0] = test_preds[:,0]*150 + 150\n",
    "test_preds[:,1] = test_preds[:,1]*-100 + 100\n",
    "test_preds = np.round(test_preds)\n",
    "print(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_x_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-785c8e3fec9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluating and printing results on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_x_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_x_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluating and printing results on test data\n",
    "test_results = model.evaluate(x=test_x_pred, y=test_y_pred)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hippoc/bin/beauvoir/data/crop_data/300x200/combined/test/0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2a90f7d8d1ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for i in range(200):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"test/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/schen-cpu/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hippoc/bin/beauvoir/data/crop_data/300x200/combined/test/0.png'"
     ]
    }
   ],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "for i in range(test_image_num):\n",
    "#for i in range(200):\n",
    "    img = Image.open(path + \"test/\"+ str(i)+'.png')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    pred = test_preds[i]\n",
    "    \n",
    "    draw.line([tuple(pred - [10, 0]), tuple(pred + [10, 0])], fill=\"blue\", width=3)\n",
    "    draw.line([tuple(pred - [0, 10]), tuple(pred + [0, 10])], fill=\"red\", width=3)\n",
    "\n",
    "#    draw.point(labels_orig[i], fill=\"blue\")\n",
    "#    draw.point(preds[i-12000], fill=\"black\")\n",
    "    print(labels_orig[i], test_preds[i], labels_orig[i]- test_preds[i])\n",
    "    img.save(path + \"test/\" + \"preds_cpu/\" + str(i) + \"_pred.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL IF IT'S GOOD\n",
    "model.save_weights('/home/hippoc/schen/models/300_200_resnet8_gates_model_300k_w_noise.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hippoc/bin/beauvoir/data/crop_data/300x200/combined/new/0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-93d9145c6b7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"new/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/schen-cpu/lib/python3.7/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/schen-cpu/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hippoc/bin/beauvoir/data/crop_data/300x200/combined/new/0.png'"
     ]
    }
   ],
   "source": [
    "# making annotations on random data\n",
    "\n",
    "#image_num = validation_labels[1] - validation_labels[0]\n",
    "image_num = 100\n",
    "images = np.zeros((image_num, input_shape[0], input_shape[1], input_shape[2]))\n",
    "for i in range(image_num):\n",
    "    img = preprocess.load_img(path + \"new/\" + str(i)+'.png')\n",
    "    img_array = preprocess.img_to_array(img)\n",
    "    images[i] = img_array\n",
    "    \n",
    "# preprocessing images (range from 0 to 1)\n",
    "images /= 255.0\n",
    "\n",
    "\n",
    "x_pred = images\n",
    "preds = model.predict(x_pred, verbose=1)\n",
    "\n",
    "preds[:,0] = preds[:,0]*150 + 150\n",
    "preds[:,1] = preds[:,1]*-100 + 100\n",
    "preds = np.round(preds)\n",
    "\n",
    "from PIL import ImageDraw\n",
    "\n",
    "for i in range(image_num):\n",
    "    img = Image.open(path + \"new/\" + str(i)+'.png')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    pred = preds[i]\n",
    "   \n",
    "    draw.line([tuple(pred - [10, 0]), tuple(pred + [10, 0])], fill=\"blue\", width=3)\n",
    "    draw.line([tuple(pred - [0, 10]), tuple(pred + [0, 10])], fill=\"red\", width=3)\n",
    "\n",
    "#    draw.point(labels_orig[i], fill=\"blue\")\n",
    "#    draw.point(preds[i-15840], fill=\"black\")\n",
    "    print(labels_orig[i])\n",
    "    img.save(path + \"new/\" + \"preds_cpu/\" + str(i) + \"_pred.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
